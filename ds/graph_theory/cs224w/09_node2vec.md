# Graph Representation Learning (node2vec)

## 1. Network Embedding

### 네트워크 임베딩이란?

* 네트워크에 머신러닝을 적용하면 어떤 작업들을 하게 될까?
    * Node Classification, Link Prediction 등등
* 지도 학습 계열의 머신러닝 방법론을 사용하기 위해서는 매번 피쳐 엔지니어링을 수행해야 한다 (Feature Learning)
    * Raw Network Data 를 어떻게 Feature를 통해 구조화된 데이터로 표현할 수 있을까?
    * Motif, Graphlets 등 다양한 방법들을 적용해서 Feature를 구할 수 있다
    * 하지만 되도록이면 자동으로 feature를 학습할 수 있으면 좋겠다
* Task와 독립적인 Feature 학습을 위해 네트워크의 각 노드를 벡터로 표현할 수 있다
    * Node -> Latent Vector
    * 이것을 **임베딩 (Embedding 또는 Feature Representation)** 이라고 한다

### 네트워크 임베딩이 왜 필요할까??

* 임베딩을 통해 네트워크의 노드들을 저차원 공간에 맵핑시킬 수 있다
* 이를 통해 유용한 결과들을 얻을 수 있다
    * 임베딩된 두 노드 사이의 유사도는 네트워크 상에서의 유사도를 나타낸다
    * 네트워크의 다양한 정보들이 벡터에 반영된다
    * 임베딩된 벡터를 가지고 다양한 머신러닝 방법론을 적용해 볼 수 있다

### 네트워크 임베딩은 어렵다.. 왜 어려울까??

* 최근들어 딥러닝 등을 위해 고안된 방법론들은 대체로 단순한 데이터 타입 (시퀀스, 그리드) 을 위해 설계되었다
    * Grid : CNN에서 사용하는 고정된 사이즈의 이미지
    * Sequence : RNN이나 word2vec에서 사용하는 텍스트
* 반면에 네트워크는 훨씬 복잡하다
    * 네트워크는 위상적으로 복잡한 구조를 가지고 있다. 픽셀들의 그리드는 근접한 픽셀들끼리만 연관이 있다고 (Locality) 가정할 수 있지만, 네트워크는 그렇지 않다
    * 이미지는 회전시키거나 왜곡을 가하면 변형되지만, Adjacency Matrix를 회전시키더라도 그래프의 위상적 구조는 변하지 않는다
    * 동적이거나 multimodal 인 경우가 많다


## 2. Embedding Nodes

### 노드를 임베딩하기

* 목표는 임베딩한 공간에서의 유사도가 원래 네트워크 상에서의 유사도와 비슷해지도록 학습하는 것이다
* 따라서 **(1) 원래 네트워크 상에서의 Similarity** 와 **(2) 네트워크에서 임베딩 공간으로 인코딩하는 방법을** 정의해야 한다

### 노드 임베딩 학습과정

1. **인코더를 정의한다** (네트워크 -> 임베딩 맵핑)
    * 인코딩은 가장 단순한 방식을 사용한다
    * 노드별 임베딩 결과를 담고 있는 행렬 Z를 학습하고, 특정 노드가 주어지면 Z에서 해당 노드의 임베딩 값을 가져오기만 한다 (Embedding Lookup)
    * 임베딩 행렬 Z는 (노드 개수 x 임베딩 차원수) 행렬이다
2. **노드간 유사도 함수를 정의한다**
    * 노드간의 유사도는 다양한 방식으로 정의할 수 있다
    * 예를 들면,
        * 얼마나 직접 연결되어 있는가?
        * 동일한 이웃을 가지고 있는가?
        * 네트워크 구조에서 같은 역할을 수행하는가?
        * 등등
3. **원본 네트워크에서의 유사도와 임베딩 공간에서의 유사도가 비슷해지도록 파라미터를 학습한다**


## 3. Random-walk Embedding

네트워크 상의 랜덤워크에서 노드 u와 v가 동시에 나타날 확률을 **유사도** 라고 정의하고, 이렇게 구한 유사도를 통해 임베딩을 학습할 수 있다.

### 왜 랜덤워크를 사용할까?

* 확률적인 정의를 통해 인접한 이웃뿐만 아니라 더 복잡한 이웃 관계를 포함하여 유사도를 계산할 수 있다
* 랜덤워크 상에서 동시에 발생하는 쌍에 대해서만 확인하면 되기 때문에 효율적으로 학습할 수 있다

### 어떻게 학습할까?

* 인접한 노드들이 임베딩한 공간에서도 가까이 위치하도록 학습하고자 한다
* 따라서 특정한 노드 u가 주어졌을 때 어떤 노드들이 근처에 있는지 잘 예측할 수 있다면 임베딩이 잘 되었다고 볼 수 있을 것이다

### 랜덤워크 최적화하기

* 랜덤워크를 통해 임베딩을 학습하는 과정은 다음과 같다
    * 짧고 고정된 길이의 랜덤워크를 특정 노드 u로부터 시작한다 (인접한 노드를 탐색하는 전략은 달라질 수 있다)
    * 랜덤워크를 통해 방문하게 되는 노드 집합 N을 수집한다 (랜덤 워크의 특성상 같은 노드에 여러번 방문할 수도 있다)
    * 노드 u가 주어졌을 때, u의 이웃 노드 N을 잘 예측하도록 임베딩을 학습한다
* 그런데 이웃노드를 예측하는 과정에서 사용하는 소프트맥스 항으로 인해 필요한 연산의 양이 너무 많아진다는 문제가 있다
    * 이웃노드가 될 확률을 계산하기 위해서는 특정 노드가 이웃인 경우 + 이웃이 아닌 경우 (즉, 모든 경우) 를 다 고려해야 하기 때문이다
* 연산량을 줄여 효과적으로 학습하기 위해 Negative Sampling을 사용한다
    * 이웃이 아닌 경우 중에서 k개만 랜덤하게 샘플링하여 확률값을 근사적으로 구한다
    * 이 때, 추출될 확률은 해당 노드의 degree 값에 비례하도록 한다
    * k값이 높을수록 노이즈에 강한 추정치가 되고, 이웃하지 않는 경우에 더 높은 prior를 부여하게 된다
    * 보통 k값은 5~20 사이의 값을 사용한다

### 랜덤워크의 전략

* 앞에서는 랜덤워크 결과가 주어졌을 때, 임베딩 작업을 최적화시키는 방법에 대해서 알아보았다
* 그렇다면 랜덤워크는 어떤 식으로 수행해야 좋을까?
* 단순하게 생각하면, 편향되지 않은 고정된 길이의 랜덤워크를 각 노드에 대해서 적용할 수 있다 (DeepWalk)
* 그런데 유사도를 이런 방식으로 정의하게 되면 모델링에 너무 제약이 많다는 문제가 있다
* 그렇다면 더 일반화시켜볼 수는 없을까?

### Node2vec (Biased Walks)

* 네트워크 상에서 이웃노드를 찾아내는 방법을 유연하게 정의한다면, 그래프 임베딩을 더 잘 할 수 있지 않을까?
* 이러한 목표를 위해 Node2vec 이라는 방법론을 사용한다
    * node2vec 에서는 편향된 2nd-order Random Walk를 사용하여 네트워크상에서 local 구조와 global 구조 중에 어느 쪽에 더 집중할 지 선택하도록 한다
    * 참고.
        * 1st-order random walk : node to node transition
        * 2nd-order random walk : edge to edge transition
* 그래프 상의 이웃을 찾기 위한 기본적인 두 가지 접근 방법이 있다
    * BFS (너비 우선 탐색) : Local 구조 중심, 미시적인 관계
    * DFS (깊이 우선 탐색) : Global 구조 중심, 거시적인 관계
* 두 개의 파라미터를 바탕으로 조절한다
    * **p** (Return parameter) : 이전 노드로 돌아오는 정도
    * **q** (In-out parameter) : BFS와 DFS의 비율
    * p값이 낮으면 BFS 같은 랜덤워크를 하게 되고, q값이 낮으면 DFS같은 랜덤워크를 하게 된다 (Trade Off 가 존재한다)
* 다음과 같은 과정을 통해 계산한다
    1. 랜덤워크 확률을 구한다
    2. 각 노드에서 시작하는 길이 l의 랜덤워크를 r번 시행한다
    3. SGD를 통해 node2vec을 최적화한다
    4. 참고 : *시간복잡도는 O(n) 이고, 위 세 단계는 각각 병렬적으로 처리할 수 있다*

### 임베딩 결과를 어떻게 활용할 수 있을까?

* i 번째 노드의 임베딩 결과를 `z_i` 라고 하면 다음과 같이 활용할 수 있다
* 클러스터링 / 커뮤니티 찾기 : `z_i` 에 대해서 클러스터링 한다
* 노드 분류하기 : `z_i`를 바탕으로 `f(z_i)` 의 레이블 값을 예측한다
* 링크 예측하기 : `f(z_i, z_j)` 를 바탕으로 엣지 `(i, j)` 가 존재하는지 여부를 예측한다
    * `f(z_i, z_j)` 는 임베딩 결과에 평균, 곱, 거리 등 다양한 연산을 적용해서 도출해낸 값이다

### 노드간의 유사도를 구하는 방법은 여러 가지가 존재한다

* (1) Adjacency 기반 (2) Multi-hop 유사도 정의 (3) 랜덤워크 기반 (node2vec)
* 해결하려는 문제에 따라서 높은 성능을 보이는 알고리즘이 다르다
* node2vec은 노드 분류 문제에서 더 잘 동작했고, multi-hop 방법은 링크 예측에서 더 높은 성능을 보였다
* 일반적으로 node2vec이 더 효율적이다
* 해결하고자 하는 문제에 가장 적합한 방식을 선택해야 한다


## 4. Embedding Entire Graphs

* 그래프 자체 또는 그래프의 일부분을 임베딩할 수는 없을까?
* 크게 세 가지 방법론이 존재한다
    * 방법1) 각 노드를 임베딩하고, 전체 그래프나 서브그래프에 해당하는 노드들의 임베딩 값을 합하거나 평균을 취한다
    * 방법2) 서브그래프의 노드들과 연결된 가상의 노드를 만들고, 해당 노드를 임베딩시킨다
    * 방법3) Anonymous Walk Embedding
        * 3-1) 가능한 모든 Anonymous Walk 조합을 구해서 확률분포의 형태로 그래프를 표현한다
        * 3-2) 샘플링을 통해 Anonymous Walk의 분포를 근사한다
        * 3-3) Anonymous Walk 자체를 임베딩한다
